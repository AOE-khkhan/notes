### Papers

- **Dropout** (2012, 2014)
	- **`Regulaizer`**, **`Ensemble`**
	- [arXiv (2012)](https://arxiv.org/abs/1207.0580), [arXiv (2014)](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf), [note](notes/dropout.md)
- Regularization of Neural Networks using DropConnect (2013)
	- **`Regulaizer`**, **`Ensemble`**
	- [paper](https://cs.nyu.edu/~wanli/dropc/dropc.pdf), [note](notes/dropconnect.md), [wanli_summary](https://cs.nyu.edu/~wanli/dropc/)
- Recurrent Neural Network Regularization (2014. 9)
	- **`RNN`**, **`Dropout to Non-Recurrent Connections`**
	- [arXiv](https://arxiv.org/abs/1409.2329)
- **Batch Normalization** (2015. 2)
	- **`Regulaizer`**, **`Accelerate Training`**, **`CNN`**
	- [arXiv](https://arxiv.org/abs/1502.03167), [note](notes/batch_normalization.md)
- Training Very Deep Networks (2015. 7)
	- **`Highway`**, **`LSTM-like`**
	- [arXiv](https://arxiv.org/abs/1507.06228), [note](notes/highway_networks.md)
- A Theoretically Grounded Application of Dropout in Recurrent Neural Networks (2015. 12)
	- **`Variational RNN`**, **`Dropout - RNN`**, **`Bayesian interpretation`**
	- [arXiv](https://arxiv.org/abs/1512.05287)
- Deep Networks with Stochastic Depth (2016. 3)
	- **`Dropout`**, **`Ensenble`**, **`Beyond 1000 layers`**
	- [arXiv](https://arxiv.org/abs/1603.09382), [note](notes/stochastic_depth.md)
- Adaptive Computation Time for Recurrent Neural Networks (2016. 3)
	- **`ACT`**, **`Dynamically`**, **`Logic Task`**
	- [arXiv](https://arxiv.org/abs/1603.08983)
- Layer Normalization (2016. 7)
	- **`Regulaizer`**, **`Accelerate Training`**, **`RNN`**
	- [arXiv](https://arxiv.org/abs/1607.06450), [note](notes/layer_normalization.md)
- Recurrent Highway Networks (2016. 7)
	- **`RHN`**, **`Highway`**, **`Depth`**, **`RNN`**
	- [arXiv](https://arxiv.org/abs/1607.03474), [note](notes/recurrent_highway.md)
- Using Fast Weights to Attend to the Recent Past (2016. 10)
	- **`Cognitive`**, **`Attention`**, **`Memory`**
	- [arXiv](https://arxiv.org/abs/1610.06258), [note](notes/fast_weights_attn.md)
- Professor Forcing: A New Algorithm for Training Recurrent Networks (2016. 10)
	- **`Professor Forcing`**, **`RNN`**, **`Inference Problem`**, **`Training with GAN`**
	- [arXiv](https://arxiv.org/abs/1610.09038), [note](notes/professor_forcing.md)
- Equality of Opportunity in Supervised Learning (2016. 10)
	- **`Equalized Odds`**, **`Demographic Parity`**, **`Bias`**
	- [arXiv](https://arxiv.org/abs/1610.02413), [the_morning_paper](https://blog.acolyer.org/2018/05/07/equality-of-opportunity-in-supervised-learning/)
- Categorical Reparameterization with Gumbel-Softmax (2016. 11)
	- **`Gumbel-Softmax distribution `**, **`Reparameterization`**, **`Smooth relaxation`**
	- [arXiv](https://arxiv.org/abs/1611.01144), [open_review](https://openreview.net/forum?id=rkE3y85ee)
- Understanding deep learning requires rethinking generalization (2016. 11)
	- **`Generalization Error`**, **`Role of Regularization`**
	- [arXiv](https://arxiv.org/abs/1611.03530)
- Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (2017. 1)
	- **`MoE Layer`**, **`Sparsely-Gated`**, **`Capacity`**, **`Google Brain`**
	- [arXiv](https://arxiv.org/abs/1701.06538), [note](notes/very_large_nn_moe_layer.md)
- **A simple neural network module for relational reasoning** (2017. 6)
	- **`Relational Reasoning`**, **`DeepMind`**
	- [arXiv](https://arxiv.org/abs/1706.01427), [note](notes/relational_network.md), [code](https://github.com/DongjunLee/relation-network-tensorflow)
- On Calibration of Modern Neural Networks (2017. 6)
	- **`Confidence calibration`**, **`Maximum Calibration Error (MCE)`**
	- [arXiv](https://arxiv.org/abs/1706.04599)
- When is a Convolutional Filter Easy To Learn? (2017. 9)
	- **`Conv + ReLU`**, **`Non-Gaussian Case`**, **`Polynomial Time`**
	- [arXiv](https://arxiv.org/abs/1709.06129), [open_review](https://openreview.net/forum?id=SkA-IE06W)
- mixup: Beyond Empirical Risk Minimization (2017. 10)
	- **`Data Augmentation`**, **`Vicinal Risk Minimization`**, **`Generalization`**
	- [arXiv](https://arxiv.org/abs/1710.09412), [open_review](https://openreview.net/forum?id=r1Ddp1-Rb)
- Measuring the tendency of CNNs to Learn Surface Statistical Regularities (2017. 11)
	- **`not learn High Level Semantics`**, **`learn Surface Statistical Regularities`**
	- [arXiv](https://arxiv.org/abs/1711.11561), [the_morning_paper](https://blog.acolyer.org/2018/05/29/measuring-the-tendency-of-cnns-to-learn-surface-statistical-regularities/)
- MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels (2017. 12)
	- **`MentorNet - StudentNet`**, **`Curriculum Learning`**, **`Output is Weight`**
	- [arXiv](https://arxiv.org/abs/1712.05055)
- Deep Learning Scaling is Predictable, Empirically (2017. 12)
	- **`Power-Law Exponents`**, **`Grow Training Sets`**
	- [arXiv](https://arxiv.org/abs/1712.00409), [the_morning_paper](https://blog.acolyer.org/2018/03/28/deep-learning-scaling-is-predictable-empirically/)
- Sensitivity and Generalization in Neural Networks: an Empirical Study (2018. 2)
	- **`Robustness`**, **`Data Perturbations`**, **`Survey`**
	- [arXiv](https://arxiv.org/abs/1802.08760), [open_review](https://openreview.net/forum?id=HJC2SzZCW)
- Can recurrent neural networks warp time? (2018. 2)
	- **`RNN`**, **`Learnable Gate`**, **`Chrono Initialization`**
	- [open_review](https://openreview.net/forum?id=SJcKhk-Ab)
- Spectral Normalization for Generative Adversarial Networks (2018. 2)
	- **`GAN`**, **`Training Discriminator`**, **`Constrain Lipschitz`**, **`Power Method`**
	- [open_review](https://openreview.net/forum?id=B1QRgziT-&noteId=BkxnM1TrM)
- On the importance of single directions for generalization (2018. 3)
	- **`Importance`**, **`Confusiing Neurons`**, **`Selective Neuron`**, **`DeepMind`**
	- [arXiv](https://arxiv.org/abs/1803.06959), [deepmind_blog](https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/)
- Group Normalization (2018. 3)
	- **`Group Normalization (GN)`**, **`Batch (BN)`**, **`Layer (LN)`**, **`Instance (IN)`**, **`Independent Batch Size`**
	- [arXiv](https://arxiv.org/abs/1803.08494)
- Fast Decoding in Sequence Models using Discrete Latent Variables (2018. 3)
	- **`Autoregressive`**, **`Latent Transformer`**, **`Discretization`**
	- [arXiv](https://arxiv.org/abs/1803.03382)
- Delayed Impact of Fair Machine Learning (2018. 3)
	- **`Outcome Curve`**, **`Max Profit, Demographic Parity, Equal Opportunity`**
	- [arXiv](https://arxiv.org/abs/1803.04383), [the_morning_paper](https://blog.acolyer.org/2018/08/13/delayed-impact-of-fair-machine-learning/), [bair_blog](https://bair.berkeley.edu/blog/2018/05/17/delayed-impact/)
- How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift) (2018. 5)
	- **`Smoothing Effect`**, **`BatchNormâ€™s Reparametrization`**
	- [arXiv](https://arxiv.org/abs/1805.11604)
- When Recurrent Models Don't Need To Be Recurrent (2018. 5)
	- **`Approximate`**, **`Feed-Forward`**
	- [arXiv](https://arxiv.org/abs/1805.10369), [bair_blog](http://bair.berkeley.edu/blog/2018/08/06/recurrent/)
- Relational inductive biases, deep learning, and graph networks (2018, 6)
	- **`Survey`**, **`Relation`**, **`Graph`**
	- [arXiv](https://arxiv.org/abs/1806.01261)
- Universal Transformers (2018. 7)
	- **`Transformer`**, **`Weight Sharing`**, **`Adaptive Computation Time (ACT)`**
	- [arXiv](https://arxiv.org/abs/1807.03819), [google_ai_blog](https://ai.googleblog.com/2018/08/moving-beyond-translation-with.html)