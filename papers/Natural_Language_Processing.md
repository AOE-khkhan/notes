### Papers

- **Distributed Representations of Words and Phrases and their Compositionality** (2013. 10)
	- **`Word2Vec`**, **`CBOW`**, **`Skip-gram`**
	- [arXiv](https://arxiv.org/abs/1310.4546)
- GloVe: Global Vectors for Word Representation (2014)
	- **`Word2Vec`**, **`GloVe`**, **`Co-Occurrence`**
	- [paper](https://nlp.stanford.edu/pubs/glove.pdf)
- **Convolutional Neural Networks for Sentence Classification** (2014. 8)
	- **`CNN`**, **`Classfication`**
	- [arXiv](https://arxiv.org/abs/1408.5882), [code](https://github.com/DongjunLee/text-cnn-tensorflow)
- **Neural Machine Translation by Jointly Learning to Align and Translate** (2014. 9)
	- **`Seq2Seq`**, **`Attention(Align)`**, **`Translation`**
	- [arXiv](https://arxiv.org/abs/1409.0473), [note](notes/bahdanau_attention.md), [code](https://github.com/DongjunLee/conversation-tensorflow)
- Text Understanding from Scratch (2015. 2)
	- **`CNN`**, **`Character-level`**
	- [arXiv](https://arxiv.org/abs/1506.07285)
- Ask Me Anything: Dynamic Memory Networks for Natural Language Processing (2015. 6)
	- **`Memory`**, **`QA`**, **`bAbi`**
	- [arXiv](https://arxiv.org/abs/1506.07285), [code](https://github.com/DongjunLee/dmn-tensorflow)
- Pointer Networks (2015. 6)
	- **`Seq2Seq`**, **`Attention`**, **`Combinatorial`**
	- [arXiv](https://arxiv.org/abs/1506.03134), [note](notes/pointer_network.md)
- **Skip-Thought Vectors** (2015. 6)
	- **`Sentence2Vec`**, **`Unsupervised`**
	- [arXiv](https://arxiv.org/abs/1506.06726), [note](notes/skip_thought.md)
- A Neural Conversational Model (2015. 6)
	- **`Seq2Seq`**, **`Conversation`**
	- [arXiv](https://arxiv.org/abs/1506.05869)
- Teaching Machines to Read and Comprehend (2015. 6)
	- **`Deepmind`**, **`Attention`**, **`QA`**
	- [arXiv](https://arxiv.org/abs/1506.03340), [note](notes/teaching_machine_read_and_comprehend.md)
- Effective Approaches to Attention-based Neural Machine Translation (2015. 8)
	- **`Seq2Seq`**, **`Attention`**, **`Translation`**
	- [arXiv](https://arxiv.org/abs/1508.04025), [note](notes/luong_attention.md), [code](https://github.com/DongjunLee/conversation-tensorflow)
- Character-Aware Neural Language Models (2015. 8)
	- **`CNN`**, **`Character-level`**
	- [arXiv](https://arxiv.org/abs/1508.06615)
- Neural Machine Translation of Rare Words with Subword Units (2015. 8)
	- **`Out-Of-Vocabulary`**, **`Translation`**
	- [arXiv](https://arxiv.org/abs/1508.07909), [note](notes/subword_nmt.md)
- A Diversity-Promoting Objective Function for Neural Conversation Models (2015. 10)
	- **`Conversation`**, **`Objective`**
	- [arXiv](https://arxiv.org/abs/1510.03055), [note](notes/diversity_conversation.md)
- **Multi-task Sequence to Sequence Learning** (2015. 11)
	- **`Multi-Task`**, **`Seq2Seq`**
	- [arXiv](https://arxiv.org/abs/1511.06114), [note](notes/multi_task_seq2seq.md)
- Multilingual Language Processing From Bytes (2015. 12)
	- **`Byte-to-Span`**, **`Multilingual`**, **`Seq2Seq`**
	- [arXiv](https://arxiv.org/abs/1512.00103), [note](notes/byte_to_span.md)
- Strategies for Training Large Vocabulary Neural Language Models (2015. 12)
	- **`Vocabulary`**, **`Softmax`**, **`NCE`**, **`Self Normalization`**
	- [arXiv](https://arxiv.org/abs/1512.04906), [note](notes/vocabulary_strategy.md)
- Incorporating Structural Alignment Biases into an Attentional Neural Translation Model (2016. 1)
	- **`Seq2Seq`**, **`Attention with Structural Biases`**, **`Translation`**
	- [arXiv](https://arxiv.org/abs/1601.01085)
- Long Short-Term Memory-Networks for Machine Reading (2016. 1)
	- **`LSTMN`**, **`Intra-Attention`**, **`RNN`**
	- [arXiv](https://arxiv.org/abs/1601.06733)
- Recurrent Memory Networks for Language Modeling (2016. 1)
	- **`RMN`**, **`Memory Bank`**
	- [arXiv](https://arxiv.org/abs/1601.01272)
- Exploring the Limits of Language Modeling (2016. 2)
	- **`Google Brain`**, **`Language Modeling`**
	- [arXiv](https://arxiv.org/abs/1602.02410), [note](notes/exploring_limits_of_lm.md)
- Swivel: Improving Embeddings by Noticing What's Missing (2016. 2)
	- **`Word2Vec`**, **`Swivel `**, **`Co-Occurrence`**
	- [arXiv](https://arxiv.org/abs/1602.02215)
- Incorporating Copying Mechanism in Sequence-to-Sequence Learning (2016. 3)
	- **`CopyNet`**, **`Seq2Seq`**
	- [arXiv](https://arxiv.org/abs/1603.06393), [note](notes/copynet.md)
- Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models (2016. 4)
	- **`Translation`**, **`Hybrid NMT`**, **`Word-Char`**
	- [arXiv](https://arxiv.org/abs/1604.00788), [note](notes/nmt_hybrid_word_and_char.md)
- Adversarial Training Methods for Semi-Supervised Text Classification (2016. 5)
	- **`Regulaizer`**, **`Adversarial`**, **`Virtual Adversarial Training (Semi-Supervised)`**
	- [arXiv](https://arxiv.org/abs/1605.07725), [note](notes/adversarial_for_semi_sv_tc.md)
- SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016. 6)
	- **`DataSet`**, **`Reading Comprehension`**
	- [arXiv](https://arxiv.org/abs/1606.05250), [note](notes/squad.md), [dataset](https://rajpurkar.github.io/SQuAD-explorer/)
- Sequence-Level Knowledge Distillation (2016. 6)
	- **`Distil`**, **`Teacher-Student`**
	- [arXiv](https://arxiv.org/abs/1606.07947), [note](notes/sequence_knowledge_distillation.md)
- Attention-over-Attention Neural Networks for Reading Comprehension (2016. 7)
	- **`Attention`**, **`Cloze-style`**, **`Reading Comprehension`**
	- [arXiv](https://arxiv.org/abs/1607.04423), [note](notes/attn_over_attn_nn_rc.md)
- Recurrent Neural Machine Translation (2016. 7)
	- **`Translation`**, **`Attention (RNN)`**
	- [arXiv](https://arxiv.org/abs/1607.08725)
- An Actor-Critic Algorithm for Sequence Prediction (2016. 7)
	- **`Seq2Seq`**, **`Actor-Critic`**, **`Objective`**
	- [arXiv](https://arxiv.org/abs/1607.07086), [note](notes/actor_critic_for_seq.md)
- Pointer Sentinel Mixture Models (2016. 9)
	- **`Language Modeling`**, **`Rare Word`**, **`Salesforce`**
	- [arXiv](https://arxiv.org/abs/1609.07843), [note](notes/ps-lstm.md)
- Multiplicative LSTM for sequence modelling (2016. 10)
	- **`mLSTM`**, **`Language Modeling`**,  **`Character-Level`**
	- [arXiv](https://arxiv.org/abs/1609.07959)
- Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models (2016. 10)
	- **`Diverse`**, **`DBS`**
	- [arXiv](https://arxiv.org/abs/1610.02424), [note](notes/dbs.md)
- Fully Character-Level Neural Machine Translation without Explicit Segmentation (2016. 10)
	- **`Translation`**, **`CNN`**, **`Character-Level`**
	- [arXiv](https://arxiv.org/abs/1610.03017), [note](notes/fully_conv_nmt.md)
- **Neural Machine Translation in Linear Time** (2016. 10)
	- **`ByteNet`**, **`WaveNet + PixelCNN`**, **`Translation`**, **`Character-Level`**
	- [arXiv](https://arxiv.org/abs/1610.10099), [note](notes/bytenet.md)
- Bidirectional Attention Flow for Machine Comprehension (2016. 11)
	- **`QA`**, **`BIDAF`**, **`Machine Comprehension`**
	- [arXiv](https://arxiv.org/abs/1611.01603), [note](notes/bi_att_flow.md), [code](https://github.com/DongjunLee/bi-att-flow-tensorflow)
- Dynamic Coattention Networks For Question Answering (2016. 11)
	- **`QA`**, **`DCN`**, **`Coattention Encoder`**, **`Machine Comprehension`**
	- [arXiv](https://arxiv.org/abs/1611.01604)
- Dual Learning for Machine Translation (2016. 11)
	- **`Translation`**, **`RL`**, **`Dual Learning (Two-agent)`**
	- [arXiv](https://arxiv.org/abs/1611.00179), [note](notes/dual_learning_nmt.md)
- Neural Machine Translation with Reconstruction (2016. 11)
	- **`Translation`**, **`Auto-Encoder`**, **`Reconstruction`**
	- [arXiv](https://arxiv.org/abs/1611.01874), [note](notes/nmt_with_reconstruction.md)
- Quasi-Recurrent Neural Networks (2016. 11)
	- **`QRNN`**, **`Parallelism`**, **`Conv + Pool + RNN`**
	- [arXiv](https://arxiv.org/abs/1611.01576), [note](notes/dual_learning_nmt.md)
- A recurrent neural network without chaos (2016. 12)
	- **`RNN`**, **`CFN`**, **`Dynamic`**, **`Chaos`**
	- [arXiv](https://arxiv.org/abs/1612.06212)
- Comparative Study of CNN and RNN for Natural Language Processing (2017. 2)
	- **`Systematic Comparison`**, **`CNN vs RNN`**
	- [arXiv](https://arxiv.org/abs/1702.01923)
- A Structured Self-attentive Sentence Embedding (2017. 3)
	- **`Sentence Embedding`**, **`Self-Attention`**, **`2-D Matrix`**
	- [arXiv](https://arxiv.org/abs/1703.03130), [note](notes/self_attn_sentence_embed.md)
- Dynamic Word Embeddings for Evolving Semantic Discovery (2017. 3)
	- **`Word Embedding`**, **`Temporal`**, **`Alignment`**
	- [arXiv](https://arxiv.org/abs/1703.00607), [the morning paper](https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/)
- Learning to Generate Reviews and Discovering Sentiment (2017. 4)
	- **`Sentiment`**, **`Unsupervised `**, **`OpenAI`**
	- [arXiv](https://arxiv.org/abs/1706.03762)
- Ask the Right Questions: Active Question Reformulation with Reinforcement Learning (2017. 5)
	- **`QA`**, **`Active Question Answering`**, **`RL`**, **`Agent (Reformulate, Aggregate)`**
	- [arXiv](https://arxiv.org/abs/1705.07830), [open_review](https://openreview.net/forum?id=S1CChZ-CZ)
- Reinforced Mnemonic Reader for Machine Reading Comprehension (2017. 5)
	- **`QA`**, **`Mnemonic (Syntatic, Lexical)`**, **`RL`**, **`Machine Comprehension`**
	- [arXiv](https://arxiv.org/abs/1705.02798)
- **Attention Is All You Need** (2017. 6)
	- **`Self-Attention`**, **`Seq2Seq (without RNN, CNN)`**
	- [arXiv](https://arxiv.org/abs/1706.03762), [note](notes/transformer.md), [code](https://github.com/DongjunLee/transformer-tensorflow)  
- Depthwise Separable Convolutions for Neural Machine Translation (2017. 6)
	- **`SliceNet`**, **`Super-Separable Conv`**, **`Depsewise + Conv 1x1`**
	- [arXiv](https://arxiv.org/abs/1706.03059), [open_review](https://openreview.net/forum?id=S1jBcueAb), [note](notes/slicenet.md)
- MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension (2017. 7)
	- **`MEMEN`**, **`QA(MC)`**, **`Embedding(skip-gram)`**, **`Full-Orientation Matching`**
	- [arXiv](https://arxiv.org/abs/1707.09098)
- On the State of the Art of Evaluation in Neural Language Models (2017. 7)
	- **`Standard LSTM`**, **`Regularisation`**, **`Hyperparemeter`**
	- [arXiv](https://arxiv.org/abs/1707.05589)
- Text Summarization Techniques: A Brief Survey (2017. 7)
	- **`Summarization`**, **`Survey`**
	- [arXiv](https://arxiv.org/abs/1707.02268), [note](notes/text_sum_survey.md)
- Adversarial Examples for Evaluating Reading Comprehension Systems (2017. 7)
	- **`Concatenative Adversaries(AddSent, AddOneSent)`**, **`SQuAD`**
	- [arXiv](https://arxiv.org/abs/1707.07328)
- Learned in Translation: Contextualized Word Vectors (2017. 8)
	- **`Word Embedding`**, **`CoVe`**, **`Context Vector`**
	- [arXiv](https://arxiv.org/abs/1708.00107)
- Simple and Effective Multi-Paragraph Reading Comprehension (2017. 10)
	- **`Document-QA`**, **`Select Paragraph-Level`**, **`Confidence Based`**, **`AllenAI`**
	- [arXiv](https://arxiv.org/abs/1710.10723), [note](notes/multi_paragraph_rc.md)
- Unsupervised Neural Machine Translation (2017. 10)
	- **`Train with both direction (tandem)`**, **`Shared Encoder`**, **`Denoising Auto-Encoder`**
	- [arXiv](https://arxiv.org/abs/1710.11041), [open_review](https://openreview.net/forum?id=Sy2ogebAW)
- Word Translation Without Parallel Data (2017. 10)
	- **`Unsupervised`**, **`Multilingual Embedding`**, **`Parallel Dictionary Induction`**
	- [arXiv](https://arxiv.org/abs/1710.04087), [open_review](https://openreview.net/forum?id=H196sainb)
- Unsupervised Machine Translation Using Monolingual Corpora Only (2017. 11)
	- **`Unsupervised`**, **`Adversarial`**, **`Monolingual Corpora`**
	- [arXiv](https://arxiv.org/abs/1711.00043), [open_review](https://openreview.net/forum?id=rkYTTf-AZ)
- Neural Text Generation: A Practical Guide (2017. 11)
	- **`Seq2Seq`**, **`Decoder Guide`**
	- [arXiv](https://arxiv.org/abs/1711.09534), [note](notes/neural_text_generation.md)
- Breaking the Softmax Bottleneck: A High-Rank RNN Language Model (2017. 11)
	- **`MoS (Mixture of Softmaxes)`**, **`Softmax Bottleneck`**
	- [arXiv](https://arxiv.org/abs/1711.03953)
- Neural Speed Reading via Skim-RNN (2017. 11)
	- **`Skim-RNN`**, **`Speed Reading`**, **`Big(Read)-Small(Skim)`**, **`Dynamic`**
	- [arXiv](https://arxiv.org/abs/1711.09534), [open_review](https://openreview.net/forum?id=Sy-dQG-Rb)
- Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks (2017. 11)
	- **`SCAN`**, **`Compositional`**, **`Mix-and-Match`**
	- [arXiv](https://arxiv.org/abs/1711.00350)
- The NarrativeQA Reading Comprehension Challenge (2017. 12)
	- **`NarrativeQA`**, **`Dataset`**, **`DeepMind`**
	- [arXiv](https://arxiv.org/abs/1712.07040), [dataset](https://github.com/deepmind/narrativeqa)
- Hierarchical Text Generation and Planning for Strategic Dialogue (2017. 12)
	- **`End2End Strategic Dialogue`**, **`Latent Sentence Representations`**, **`Planning + RL`**
	- [arXiv](https://arxiv.org/abs/1712.05846)
- Recent Advances in Recurrent Neural Networks (2018. 1)
	- **`RNN`**, **`Recent Advances`**, **`Review`**
	- [arXiv](https://arxiv.org/abs/1801.01078)
- Personalizing Dialogue Agents: I have a dog, do you have pets too? (2018. 1)
	- **`Chit-chat`**, **`Profile Memory`**, **`Persona-Chat Dataset`**, **`ParlAI`**
	- [arXiv](https://arxiv.org/abs/1801.07243)
- Generating Wikipedia by Summarizing Long Sequences (2018. 1)
	- **`Multi-Document Summarization`**, **`Extractive-Abstractive Stage`**, **`T-DMCA`**, **`WikiSum`**, **`Google Brain`**
	- [arXiv](https://arxiv.org/abs/1801.10198), [note](notes/generate_wiki.md), [open_review](https://openreview.net/forum?id=Hyg0vbWC-)
- MaskGAN: Better Text Generation via Filling in the______ (2018. 1)
	- **`MaskGAN`**, **`Neural Text Generation`**, **`RL Approach`**
	- [arXiv](https://arxiv.org/abs/1801.07736), [open_review](https://openreview.net/forum?id=ByOExmWAb&noteId=HJbx71pBM), [note](notes/mask_gan.md)
- Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs (2018. 1)
	- **`Contextual Decomposition (CD)`**, **`Disambiguate interactions between Gates`**
	- [arXiv](https://arxiv.org/abs/1801.05453), [open_review](https://openreview.net/forum?id=rkRwGg-0Z)
- DeepType: Multilingual Entity Linking by Neural Type System Evolution (2018. 2)
	- **`DeepType`**, **`Symbolic Information`**, **`Type System`**, **`Open AI`**
	- [arXiv](https://arxiv.org/abs/1802.01021), [openai blog](https://blog.openai.com/discovering-types-for-entity-disambiguation/)
- Deep contextualized word representations (2018. 2)
	- **`biLM`**, **`ELMo`**, **`Word Embedding`**, **`Contextualized`**, **`AllenAI`**
	- [arXiv](https://arxiv.org/abs/1802.05365), [note](notes/contextualized_word_for_rc.md)
- Ranking Sentences for Extractive Summarization with Reinforcement Learning (2018. 2)
	- **`Document-Summarization`**, **`Cross-Entropy vs RL`**, **`Extractive`**
	- [arXiv](https://arxiv.org/abs/1802.08636)
- code2vec: Learning Distributed Representations of Code (2018. 3)
	- **`code2vec`**, **`Code Embedding`**, **`Predicting method name`**
	- [arXiv](https://arxiv.org/abs/1803.09473)
- Universal Sentence Encoder (2018. 3)
	- **`Transformer`**, **`Deep Averaging Network (DAN)`**, **`Transfer`**
	- [arXiv](https://arxiv.org/abs/1803.11175)
- An efficient framework for learning sentence representations (2018. 3)
	- **`Sentence Representation`**, **`True Context`**, **`Unsupervised`**
	- [arXiv](https://arxiv.org/abs/1803.02893), [open_review](https://openreview.net/forum?id=rJvJXZb0W)
- An Analysis of Neural Language Modeling at Multiple Scales (2018. 3)
	- **`LSTM vs QRNN`**, **`Hyperparemeter`**, **`AWD-QRNN`**
	- [arXiv](https://arxiv.org/abs/1803.08240)
- Analyzing Uncertainty in Neural Machine Translation (2018. 3)
	- **`Uncertainty`**, **`Beam Search Degradation`**, **`Copy Mode`**
	- [arXiv](https://arxiv.org/abs/1803.00047)
- An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (2018. 3)
	- **`Temporal Convolutional Network (TCN)`**, **`CNN vs RNN`**
	- [arXiv](https://arxiv.org/abs/1803.01271)
- Training Tips for the Transformer Model (2018. 4)
	- **`Transformer`**, **`Hyperparameter`**, **`Multiple GPU`**
	- [arXiv](https://arxiv.org/abs/1804.00247)
- QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension (2018. 4)
	- **`QA`**, **`Conv - Self-Attention`**, **`Backtranslation (Data Augmentation)`**
	- [arXiv](https://arxiv.org/abs/1804.09541), [open_review](https://openreview.net/forum?id=B14TlG-RW), [note](notes/qanet.md)
-  SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach (2018. 4)
   - **`Top-K Subject Recognitio`**, **`Relation Classification`**
	- [arXiv](https://arxiv.org/abs/1804.08798)
-  Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer (2018. 4)
   - **`Sentiment Transfer`**, **`Disentangle Attribute`**, **`Unsupervised`** 
	- [arXiv](https://arxiv.org/abs/1804.06437)
- Parsing Tweets into Universal Dependencies (2018. 4)
	- **`Universal Dependencies (UD)`**, **`TWEEBANK v2`**
	- [arXiv](https://arxiv.org/abs/1804.08228)
- Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (2018. 4)
	- **`SR`**, **`Subword Sampling + Hyperparameter`**, **`Segmentation (BPE, Unigram)`**
	- [arXiv](https://arxiv.org/abs/1804.10959) 
- Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension (2018. 4)
	- **`PI-SQuAD`**, **`Challenge`**, **`Document Encoder`**, **`Scalability`**
	- [arXiv](https://arxiv.org/abs/1804.07726)
- On the Practical Computational Power of Finite Precision RNNs for Language Recognition (2018. 5)
	- **`Unbounded counting`**, **`IBFP-LSTM`**
	- [arXiv](https://arxiv.org/abs/1805.04908)
- Paper Abstract Writing through Editing Mechanism (2018. 5)
	- **`Writing-editing Network`**, **`Attentive Revision Gate`**
	- [arXiv](https://arxiv.org/abs/1805.06064)
- A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings (2018. 5)
	- **`Unsupervised initialization scheme`**, **`Robust self-leraning`**
	- [arXiv](https://arxiv.org/abs/1805.06297)
- Efficient and Robust Question Answering from Minimal Context over Documents (2018. 5)
	- **`Sentence Selector`**, **`Oracle Sentence`**, **`Minimal Set of Sentences (SpeedUp)`**
	- [arXiv](https://arxiv.org/abs/1805.08092), [note](notes/minimal_qa.md)
- Global-Locally Self-Attentive Dialogue State Tracker (2018. 5)
	- **`GLAD`**, **`WoZ and DSTC2 Dataset`**
	- [arXiv](https://arxiv.org/abs/1805.09655) 
- Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information (2018, 5)
	- **`Dataset`**, **`EVPI`**, **`ACL 2018 Best Paper`**
	- [arXiv](https://arxiv.org/abs/1805.04655)
- Know What You Don't Know: Unanswerable Questions for SQuAD (2018, 6)
	- **`SQuAD 2.0`**, **`Negative Example`**, **`ACL 2018 Best Paper`**
	- [arXiv](https://arxiv.org/abs/1806.03822), [leaderboard](https://rajpurkar.github.io/SQuAD-explorer/)
- The Natural Language Decathlon: Multitask Learning as Question Answering (2018, 6)
	- **`decaNLP`**, **`Multitask Question Answering Network (MQAN)`**, **`Transfer Learning`**
	- [arXiv](https://arxiv.org/abs/1806.08730)
- GLoMo: Unsupervisedly Learned Relational Graphs as Transferable Representations (2018, 6)
	- **`Transfer Learning Framework`**, **`Structured Graphical Representations`**
	- [arXiv](https://arxiv.org/abs/1806.08730)
- Improving Language Understanding by Generative Pre-Training (2018, 6)
	- **`Transformer`**, **`Generative Pre-Training`**, **`Discriminative Fine-Tuning`**
	- [paper](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [open\_ai\_blog](https://blog.openai.com/language-unsupervised/)
- Finding Syntax in Human Encephalography with Beam Search (2018, 6)
	- **`RNNG+beam search`**, **`ACL 2018 Best Paper`**
	- [arXiv](https://arxiv.org/abs/1806.04127)
- Let's do it "again": A First Computational Approach to Detecting Adverbial Presupposition Triggers (2018, 6)
	- **`Task`**, **`Dataset`**, **`Weighted-Pooling (WP)`** **`ACL 2018 Best Paper`**
	- [arXiv](https://arxiv.org/abs/1806.04262)
- QuAC : Question Answering in Context (2018. 8)
	- **`Information-Seeking dialog`**, **`Challenge`**, **`Without Evidence`**
	- [arXiv](https://arxiv.org/abs/1808.07036), [leaderboard](http://quac.ai/)
- CoQA: A Conversational Question Answering Challenge (2018. 8)
	- **`Abstractive with Extractive Rationale`**, **`Challenge`**, **`Coreference and Pragmatic Reasoning`**
	- [arXiv](https://arxiv.org/abs/1808.07042), [leaderboard](https://stanfordnlp.github.io/coqa/)
- Contextual Parameter Generation for Universal Neural Machine Translation (2018. 8)
	- **`Parameter Generation`**, **`Language Embedding`**, **`EMNLP 2018`**
	- [arXiv](https://arxiv.org/abs/1808.08493)
- Evaluating Theory of Mind in Question Answering (2018. 8)
	- **`Dataset`**, **`Higher-order Beliefs`**, **`EMNLP 2018`**
	- [arXiv](https://arxiv.org/abs/1808.09352)
- Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text (2018. 9)
	- **`GRAFT-Net`**, **`KB+Text Fusion`**, **`EMNLP 2018`**
	- [arXiv](https://arxiv.org/abs/1809.00782)
- HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering (2018. 9)
	- **`Dataset`**, **`Multi-hop`**, **`Sentence-level Supporting Fact`**, **`EMNLP 2018`**
	- [arXiv](https://arxiv.org/abs/1809.00782)
- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018. 10)
	- **`BERT`**, **`Discriminative`**, **`Large Pretrained`**, **`Transfer Learning`**
	- [arXiv](https://arxiv.org/abs/1810.04805), [glue_benchmark](https://gluebenchmark.com/leaderboard)